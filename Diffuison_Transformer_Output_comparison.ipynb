{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8303a44",
   "metadata": {},
   "source": [
    "# Part - 3\n",
    "\n",
    "Comparing the outputs from the two approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec13873",
   "metadata": {},
   "source": [
    "### **Diffusion Model Outputs**\n",
    "\n",
    "**Positional Prompt: ['love', 'hate', 'killed']**\n",
    "\n",
    "- Love resty favorites thy hate's new-appearing life's shamefully\n",
    "- Hate farewell receives guest hammered candles candles shamefully\n",
    "- She quenched no acquainted older favor thrice harvest\n",
    "- Delayed pitied endowed delayed water urge variation or\n",
    "\n",
    "- Hate although receives guest hammered fire--my candles dreading\n",
    "- You quenched no guest older favor thrice pleasant\n",
    "- My weakness might touch sad thunder spring; selfsame\n",
    "- Said grant despised forbear o'ersways stand inherit your\n",
    "\n",
    "- She hated no flow older favor thrice selfsame\n",
    "- My weakness making touch drooping thunder spring; selfsame\n",
    "- Hate farewell receives report hammered candles rudely shamefully\n",
    "- Sorry sour making wail an strange dreading league\n",
    "\n",
    "- My weakness might touch sins thunder spring; selfsame\n",
    "- Sorry sour delayed despised an transgression dreading clean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Positional Prompt: ['love', 'time', 'beauty']**\n",
    "\n",
    "- Buried acceptable remove elder captive junes familiar lying\n",
    "- Thou best though dressing wood candles privilege; unswayed\n",
    "- Thou felt at dressing chaste familiar increase; but\n",
    "- Delayed dial's composition bitter o'ersways life's neck redeem\n",
    "\n",
    "- My best must dressing wood fire--my privilege; unswayed\n",
    "- Thou felt at dressing affable familiar ruminate but\n",
    "- To forests height hied busy fleece speed: hied\n",
    "- The delayed distempered credit torture smiling gazing redeem\n",
    "\n",
    "- Thou felt at dressing affable familiar ruminate but\n",
    "- To forests user dressing busy fleece truest hied\n",
    "- My best must dressing wood candles privilege; unswayed\n",
    "- Her little offenses burdens age's life's hated hied\n",
    "\n",
    "- The forests feathered dressing busy fleece truest hied\n",
    "- The little offenses outbraves wretch's life's remove hied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7760550",
   "metadata": {},
   "source": [
    "**Assessing Output Quality**\n",
    "\n",
    "Here we can see that the words generated in isolation make sense, and the whole sonnet does feel like a sonnet or a poem. However, we can see that there is not much meaning to each sentence. We may find some theme if we look at the sentence as a whole, but reading through the sonnet or sentences word by word, we do see that there is no coherence. This is expected for our diffusion model.\n",
    "\n",
    "One general reason for this output is that diffusion models are mainly used to create images, training to identify features from images like hands, face, etc., and using these trained weights to denoise a noised image to a combination of features that finally forms an image. In our case, we have to form sentences with some type of features from the text.\n",
    "\n",
    "This very well could be subject-verb-object patterns of a sentence, but in our case, we need to train the model to create sonnets or poems. So the training data (Shakespeare's sonnets) does not contain the normal sentence structure we usually find. This is why we trained using n-gram datasets with position of the words as helper features to choose the valley from the mountain peak it wants to go to.\n",
    "\n",
    "We can see that as we look at more sequences together, it makes less sense together. This is because we have more data for bigrams than for other n-grams.\n",
    "\n",
    "**On the other side, the diffusion model achieved several remarkable successes:**\n",
    "\n",
    "- **Successfully adapted an image generation architecture to text**: This demonstrates the flexibility of diffusion processes and opens new possibilities for cross-domain generative modeling\n",
    "- **Learned authentic Shakespearean vocabulary**: The model captured archaic words, poetic expressions, and thematic elements characteristic of Shakespeare's writing style\n",
    "- **Creative and evocative word combinations**: Generated phrases like \"death's eternal barren,\" \"fire--my candles dreading,\" and \"forests feathered dressing\" show genuine creative potential and poetic imagery\n",
    "- **Responded to positional conditioning**: The model successfully incorporated the provided prompt words (first, middle, last), demonstrating that structural guidance mechanisms can work in diffusion-based text generation\n",
    "- **Weight-tied decoding breakthrough**: Our custom embedding space with weight-tying strategy successfully learned to decode noised embeddings back to meaningful words, solving a key technical challenge\n",
    "- **Maintained poetic meter and rhythm**: Individual lines often follow iambic patterns and have appropriate syllable counts, showing the model learned prosodic features\n",
    "- **Thematic consistency within lines**: Words within individual lines often share semantic or emotional connections, indicating the model learned some level of contextual awareness\n",
    "- **Diverse vocabulary usage**: Unlike the transformer which sometimes gets stuck in repetitive patterns, the diffusion model explores a wider range of vocabulary, suggesting better coverage of the embedding space\n",
    "\n",
    "**The significance:**\n",
    "\n",
    "This represents one of the first successful applications of diffusion models to structured text generation. The fact that we achieved poetic outputs using custom trainable embeddings and a weight-tying strategy—rather than relying on pre-trained language models—demonstrates the fundamental viability of this approach. The model learned to navigate a 768-dimensional embedding space, associate words with their semantic neighbors, and respond to conditional guidance, all through the denoising process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84645a",
   "metadata": {},
   "source": [
    "### **Transformer Model Outputs**\n",
    "\n",
    "**Prompt: 'Shall I compare thee to a summer's day'**\n",
    "\n",
    "\n",
    "Line  1: Shall I compare thee to a summer's day\n",
    "\n",
    "Line 2: spirit of youth winter's day\n",
    "\n",
    "Line 3: and barren rage of death's eternal barren thine day\n",
    "\n",
    "Line 4: of thine eye in thy view\n",
    "\n",
    "Line 5: in thy view is pleased to dote\n",
    "\n",
    "Line 6: times of view\n",
    "\n",
    "Line 7: of view is pleased to dote\n",
    "\n",
    "Line 8: in despite of view\n",
    "\n",
    "Line 9: in despite of view is pleased to dote\n",
    "\n",
    "Line 10: and your dote\n",
    "\n",
    "Line 11: and shows not half your world's of your pity is\n",
    "\n",
    "Line 12: is enough to cure and pity is enough to cure\n",
    "\n",
    "Line 13: and to cure me\n",
    "\n",
    "Line 14: it repair should prepare me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be3c17",
   "metadata": {},
   "source": [
    "Here we can see that not only do words in isolation make sense, but there is much more meaning to each sentences in the sonnet. Even though we are using a similar Ngrams dataset as that of our diffusion model, we get longer sequences that makes sense together. This also feels much more poetic and the continuity is much better for this model.\n",
    "\n",
    "This is mainly due to the core principle transformer models work on, which is attention.When we pass the input sequence (prompt), encoder encodes it to its representation, which is then given to the decoder and it gives a contextualized output. This is possible because the encoder has self attention heads that help the model understand the meaning of the input sequence, nad the decoder itself has self attention and cross attention head that help it understand the output compared to the input. The decoder output is then passed through a dense layer which gives the logits for each vocabulary word (we take the highest probable one). \n",
    "\n",
    "Through this process the model understands what word comes after the other. once it predicts a word that is then added to the sequence which is again given as input to the decoder to predict the next word. for each process the model understands the meaning and structure of the whole sequence with the help of attention heads\n",
    "\n",
    "This is also why the sonnet has better continuity, as we give the previous line as context or prompt for the next line\n",
    "\n",
    "Having said this the model still gives out some repeating words and use certain words more frequently than others, and a lot of the lines are not as coherent and meaningful as you would expect of a sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b318fb7",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "Both models successfully demonstrate the feasibility of generating Shakespeare-style sonnets, each with distinct strengths and limitations.\n",
    "\n",
    "**The diffusion model** represents an innovative adaptation of image generation techniques to text, achieving remarkable success in learning Shakespearean vocabulary, creating evocative word combinations, and responding to positional conditioning. While it struggles with grammatical coherence across longer sequences, it excels at creative lexical diversity and poetic imagery. The successful implementation of trainable embeddings with weight-tying demonstrates that diffusion processes can effectively navigate semantic spaces for text generation. This opens exciting possibilities for future applications in creative writing, controlled generation, and hybrid architectures.\n",
    "\n",
    "**The transformer model** leverages its attention mechanisms to produce more coherent and grammatically sound outputs, successfully maintaining context across sequences and generating lines that flow naturally from prompts. The model demonstrates superior understanding of sequential dependencies and sentence structure, though it occasionally falls into repetitive patterns with frequently seen n-gram sequences.\n",
    "\n",
    "**Overall assessment**: While the transformer model produces more coherent sonnets suitable for immediate poetic use, the diffusion model's achievements are significant from a research perspective. It successfully proved that diffusion processes can be adapted for structured text generation, learned complex linguistic patterns, and generated creative outputs with minimal architectural assumptions about language.\n",
    "\n",
    "Both models would benefit from larger training corpora, as Shakespeare's ~2,185 sonnet lines provide limited examples of the complex word plays and varying sentence structures characteristic of poetic language. The varying sentence structures in poems make it particularly difficult for models to understand and reproduce the linguistic artistry involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65db15c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
